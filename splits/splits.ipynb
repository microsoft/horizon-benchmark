{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5126308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 53514510 records from ../amazon_parquet_data/merged_users_all_final_filtered_no_reviews.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "main_json = '../amazon_parquet_data/merged_users_all_final_filtered_no_reviews.json'\n",
    "with open(main_json, 'r') as f:\n",
    "    data = json.load(f)\n",
    "print(f\"Loaded {len(data)} records from {main_json}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d6d7c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ind_test_user_ids_sampled.txt...\n",
      "Loaded 25000 user IDs from ind_test_user_ids_sampled.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1198107/53514510 [00:00<00:20, 2601447.54it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53514510/53514510 [00:17<00:00, 3106759.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data contains 25000 records\n",
      "Saved filtered data to ind_test_user_ids_sampled.json\n",
      "Processing ind_train_user_ids_sampled.txt...\n",
      "Loaded 150000 user IDs from ind_train_user_ids_sampled.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53514510/53514510 [00:19<00:00, 2693724.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data contains 150000 records\n",
      "Saved filtered data to ind_train_user_ids_sampled.json\n",
      "Processing ind_val_user_ids_sampled.txt...\n",
      "Loaded 25000 user IDs from ind_val_user_ids_sampled.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53514510/53514510 [00:18<00:00, 2866441.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data contains 25000 records\n",
      "Saved filtered data to ind_val_user_ids_sampled.json\n",
      "Processing ood_test_user_ids_sampled.txt...\n",
      "Loaded 25000 user IDs from ood_test_user_ids_sampled.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53514510/53514510 [00:18<00:00, 2920844.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data contains 25000 records\n",
      "Saved filtered data to ood_test_user_ids_sampled.json\n",
      "Processing ood_val_user_ids_sampled.txt...\n",
      "Loaded 25000 user IDs from ood_val_user_ids_sampled.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53514510/53514510 [00:18<00:00, 2937813.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data contains 25000 records\n",
      "Saved filtered data to ood_val_user_ids_sampled.json\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "SAMPLED_FILE_NAMES = [\n",
    "    'ind_test_user_ids_sampled.txt',\n",
    "    'ind_train_user_ids_sampled.txt',\n",
    "    'ind_val_user_ids_sampled.txt',\n",
    "    'ood_test_user_ids_sampled.txt',\n",
    "    'ood_val_user_ids_sampled.txt',\n",
    "]\n",
    "for file_name in SAMPLED_FILE_NAMES:\n",
    "    print(f\"Processing {file_name}...\")\n",
    "    with open(file_name, 'r') as f:\n",
    "        sampled_user_ids = set(line.strip() for line in f)\n",
    "    print(f\"Loaded {len(sampled_user_ids)} user IDs from {file_name}\")\n",
    "\n",
    "    # Filter the main data based on the sampled user IDs\n",
    "    filtered_data = {}\n",
    "    for key, val in tqdm(data.items()):\n",
    "        if key in sampled_user_ids:\n",
    "            filtered_data[key] = val\n",
    "\n",
    "    print(f\"Filtered data contains {len(filtered_data)} records\")\n",
    "\n",
    "    # Save the filtered data to a new JSON file\n",
    "    output_file_name = file_name.replace('.txt', '.json')\n",
    "    with open(output_file_name, 'w') as f:\n",
    "        json.dump(filtered_data, f, indent=4)\n",
    "    print(f\"Saved filtered data to {output_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0408dca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 25000 records from ood_test_user_ids_sampled.json\n",
      "Loaded 25000 records from ind_test_user_ids_sampled.json\n",
      "Merged data contains 50000 records\n",
      "Saved merged data to LLM_testing_ood_ind_test_user_ids_sampled.json\n"
     ]
    }
   ],
   "source": [
    "# Merge ood and ind test sampled json files\n",
    "ood_json = 'ood_test_user_ids_sampled.json'\n",
    "ind_json = 'ind_test_user_ids_sampled.json'\n",
    "with open(ood_json, 'r') as f:\n",
    "    ood_data = json.load(f)\n",
    "print(f\"Loaded {len(ood_data)} records from {ood_json}\")\n",
    "with open(ind_json, 'r') as f:\n",
    "    ind_data = json.load(f)\n",
    "print(f\"Loaded {len(ind_data)} records from {ind_json}\")\n",
    "merged_data = {**ood_data, **ind_data}\n",
    "print(f\"Merged data contains {len(merged_data)} records\")\n",
    "# Save the merged data to a new JSON file\n",
    "output_file_name = 'LLM_testing_ood_ind_test_user_ids_sampled.json'\n",
    "with open(output_file_name, 'w') as f:\n",
    "    json.dump(merged_data, f, indent=4)\n",
    "print(f\"Saved merged data to {output_file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "157a5b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:00<00:00, 2191795.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max history length: 8609\n",
      "Min history length: 1\n",
      "Avg history length: 15.401108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "lengths = []\n",
    "for user_id, user_data in tqdm(data.items()):\n",
    "    history = user_data['history']\n",
    "    lengths.append(len(history))\n",
    "print(f\"Max history length: {max(lengths)}\")\n",
    "print(f\"Min history length: {min(lengths)}\")\n",
    "print(f\"Avg history length: {sum(lengths)/len(lengths)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "067d4c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000000/1000000 [00:00<00:00, 2307873.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users with history length greater than 1000: 15683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#   Get lengths greater than 1000\n",
    "lengths = []\n",
    "for user_id, user_data in tqdm(data.items()):\n",
    "    history = user_data['history']\n",
    "    if len(history) > 100:\n",
    "        lengths.append(len(history))\n",
    "print(f\"Number of users with history length greater than 1000: {len(lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9950526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered data saved to amazon-benchmark-recsys-new/amazon_parquet_data/filtered_users_all_final_filtered_no_reviews.json\n"
     ]
    }
   ],
   "source": [
    "# save the filtered data to a new JSON file\n",
    "output_path = 'amazon-benchmark-recsys-new/amazon_parquet_data/filtered_users_all_final_filtered_no_reviews.json'\n",
    "\n",
    "with open(output_path, 'w') as file:\n",
    "    json.dump(data_new, file, indent=4)\n",
    "print(f\"Filtered data saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
